# biasSDG

Adversarial bias injection into tabular synthetic data generation (SDG) frameworks. This project explores how prompt-based and in-context learning (ICL) mechanisms can be manipulated to inject social biases (e.g., gender, race) into tabular data generated by large language models (LLMs).

---

## ðŸ“‚ Project Structure

```
biasSDG/
â”‚
â”œâ”€â”€ src/                        # Source code
â”‚   â”œâ”€â”€ configs/                # YAML files for config and experiments
â”‚   â”œâ”€â”€ utils/                  # Utility modules (prompting, saving, loading)
â”‚   â”œâ”€â”€ prompts/                # Base prompt templates
â”‚   â”œâ”€â”€ evaluation/             # Evaluation scripts for fidelity, utility and fairness
â”‚   â””â”€â”€ data_generation/        # Generation scripts for structured data with LLMs
â”‚   â””â”€â”€ nlp/                    # Scripts on data generation for NLP tasks
â”‚
â”œâ”€â”€ results/                    # Saved synthetic datasets and evaluation outputs
â”œâ”€â”€ notebooks/                  # Analysis notebooks (optional, WIP)
â”œâ”€â”€ figures/                    # Generated plots
â”œâ”€â”€ README.md                   # You're here
â””â”€â”€ requirements.txt            # Python dependencies
```

---

## ðŸ§  Motivation

Tabular SDG frameworks using LLMs can be compromised by subtle adversarial manipulations. This project demonstrates:

- How biased prompts or in-context examples lead to measurable bias in synthetic datasets.
- How to systematically evaluate bias severity and fidelity tradeoffs.
- The need for red/blue teaming in SDG pipelines.

---

## ðŸ›  Setup

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

### 2. Set up configs

Edit the default config files under `./src/configs/`:

- `config.yaml`: General and model-specific settings.
- `experiments.yaml`: List of experiments to run.

---

## ðŸ§ª Data Generation

To generate synthetic data using a specific set of prompts and experiment configurations:

```bash
python src/data_generation/tabular/bias_severity_rits.py \
    --config-path ./src/configs/config.yaml \
    --experiments-path ./src/configs/exp_bias_severity.yaml \
    --save True
```

This will:

- Load and optionally inject in-context examples into prompts.
- Use RITS API call to the defined LLM.
- Save generated data to the output path defined in your config.

The synthetic data is going to be stored at output_data/ folder path, specified in the config file. The path indicates the model type, the gender icl configuration, and the prompt number.

---